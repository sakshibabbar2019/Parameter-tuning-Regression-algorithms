{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79SZ2Uhwd9aO"
      },
      "source": [
        "# Parameter Tuning of Regression Algorithms\n",
        "\n",
        "Regression algorithms are often parameterized. These parameters controls the behaviour and performance of the model. Models can have many parameters and finding the best combination of parameters is known as $\\textbf{Hyperparameter optimization}$. The process of hyperparamter optimization uses a search strategy to find a robust parameters for a model on a given problem. \n",
        "\n",
        "Scikit learn library is one choice of modeling regression algorithms. The regression models built in scikit learn library comes with default parameter setting. However, these settings can be altered to find the best paramter combinations so as to get good performance by the models. In this notebook,  process of hyperparameter optimization using scikit learn library on different regression models is dicussed. Following  linear and non linear regression models will be used for demonstration\n",
        "\n",
        "1. Linear models\n",
        "    - Ridge regression\n",
        "        * parameter: \n",
        "              ** alpha: Regularization strength.It must be a positive float. The default \n",
        "                 value is 1. \n",
        "    - Lasso regression\n",
        "        * parameter: \n",
        "              ** alpha: Regularization strength.It must be a positive float. The default \n",
        "                 value is 1. \n",
        "    - Elastic net regression\n",
        "        * parameter: \n",
        "              ** alpha: Regularization strength.It must be a positive float. The default \n",
        "                 value is 1. \n",
        "\n",
        "2. Non-linear models\n",
        "    - Classification and Regression Trees(CART): Decision tree regressor\n",
        "        * parameters: \n",
        "                ** max_depth : This indicates how deep the tree can be. The deeper the \n",
        "                   tree, the more splits it has and it captures more information about the \n",
        "                   data. The default value of None.  \n",
        "                ** min_samples_split: It represents the minimum number of samples required \n",
        "                   to split an internal node. This can vary between considering at least \n",
        "                   one sample at each node to considering all of the samples at each node. \n",
        "                   When we increase this parameter, the tree becomes more constrained as \n",
        "                   it has to consider more samples at each node. Here we will vary the \n",
        "                   parameter from 10% to 100% of the samples. The default value is 2.  \n",
        "                ** min_samples_leaf: The minimum number of samples required to be at a \n",
        "                   leaf node. This parameter is similar to min_samples_splits, however, \n",
        "                   this describe the minimum number of samples of samples at the leafs,the \n",
        "                   base of the tree. The default value is 1.  \n",
        "                ** max_leaf_nodes: It defines maxinum leaf nodes in the model. The default \n",
        "                   value of None. \n",
        "    - Support vector regressor\n",
        "        * parameters:\n",
        "                ** kernel: Specifies the kernel type to be used in the algorithm either \n",
        "                   \"linear\" or \"rbf\". The default setting of kernel is rbf.\n",
        "                ** C : Penalty parameter C of the error term. The default value is 1.0\n",
        "                ** gamma : Kernel coefficient for ‘rbf’. The default value is 1/n where, n \n",
        "                   is number of features in data set.\n",
        "          \n",
        "    - K nearest neighbor regressor\n",
        "        * parameters:\n",
        "                ** n_neighbors: Number of neighbors to use. The default value is 5. \n",
        "                ** metric : Distance metric to use. The default value is \"Minkowski\"\n",
        "                \n",
        "Scikit learn provides Grid search method to be used to find optimal parameter combinations for these models. The method is available under sklearn.model_selection. The name of the method under this library is GridSearchCV(). This method applies exhaustive search over specified parameters values organized in a grid to fit the model using  cross validation technique. In other words, each cross validation run takes pair of parameter values organized in a grid to train  and evaluate the model. The parameter values on which the cross validation results in best performance is considered as final choice of parameters for the model.  \n",
        "\n",
        "The data set used for demonstration is Moneyball which can downloaded form https://www.kaggle.com/wduckett/moneyball-mlb-stats-19622012/data . The data has been gathered from baseball-reference.com. It contains following features:\n",
        "\n",
        "1. RA: runs allowed\n",
        "2. RS:  runs scored\n",
        "3. OBP: On Base Percentage\n",
        "4. SLG: Slugging Percentage\n",
        "5. BA: Batting Average\n",
        "6. OOBP: opponent’s OBP\n",
        "7. OSLG: opponent’s SLG\n",
        "8. W:  wins in that season\n",
        "\n",
        "The features from 1-7 are used as indicator variables to predict the outcome W(i.e., wins in season). \n",
        "\n",
        "The step by step practical learning on hyperparamter optimization of regression models is demonstrated below. Please note that aim of this tutorial is not to find the best regression model for the predictive task. Rather the aim is to learn how to tune paramters of the model during the training phase so as to get robust model for making predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJp1rP54d9aR"
      },
      "source": [
        "# 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRh4NAe2d9aS"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Pandas for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# importing linear and non linear regression models\n",
        "from sklearn import linear_model  \n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# import GridSearchCV method\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "GzjV6VYBgbCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-MIoNrOd9aT"
      },
      "source": [
        "# 2. Load data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AM8fUNgd9aT",
        "outputId": "ff778f0a-576c-4d58-cc7c-dda41ac98db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 902 entries, 0 to 901\n",
            "Data columns (total 8 columns):\n",
            "RS          902 non-null int64\n",
            "RA          902 non-null int64\n",
            "OBP         902 non-null float64\n",
            "SLG         902 non-null float64\n",
            "BA          902 non-null float64\n",
            "Playoffs    902 non-null int64\n",
            "RD          902 non-null int64\n",
            "W           902 non-null int64\n",
            "dtypes: float64(3), int64(5)\n",
            "memory usage: 56.5 KB\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_csv(\"Data sets/moneyball.csv\")\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCeofoTed9aU"
      },
      "source": [
        "# 3. Building training and test sets\n",
        "\n",
        "The training set will be used  by GridSearchCV() method to find best combination of parameters for the given data set. Once the parameters are tuned, the model is tested on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhwAuH3Id9aV",
        "outputId": "1a7ad20d-8727-46b3-fddd-f107c1054032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      RS   RA    OBP    SLG     BA  Playoffs   RD\n",
            "896  817  680  0.337  0.426  0.267         1  137\n",
            "542  600  750  0.315  0.363  0.244         0 -150\n",
            "255  684  622  0.322  0.400  0.257         1   62\n",
            "412  632  655  0.317  0.361  0.247         0  -23\n",
            "328  594  583  0.310  0.351  0.247         0   11\n",
            "437    89\n",
            "131    84\n",
            "633    82\n",
            "195    68\n",
            "230    94\n",
            "Name: W, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# My_data contains all data points from My_data set from from first feature to 6th feature(indicator features)\n",
        "My_data = dataset.iloc[:,0:7] \n",
        "\n",
        "# My_target contains class information which is 7th feature in the data set of \n",
        "\n",
        "My_data_target=dataset.iloc[:,7]\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(My_data, My_data_target, test_size=0.8, random_state=10)\n",
        "\n",
        "print(X_train.head())\n",
        "print(Y_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak1M7gp8d9aY"
      },
      "source": [
        "# 4. Tuning Linear models \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKE2lqj1d9aY"
      },
      "source": [
        "In following codes Ridge, Lasso and Elastic net regression models are tuned on parameter alpha.\n",
        "\n",
        "\n",
        "The user defined different values to alpha are first set and then these values are exposed to GridSearchCV() to find the best alpha value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHu6nOXpd9aZ"
      },
      "outputs": [],
      "source": [
        "# specifying alpha values\n",
        "alphas =[0.2,0.4,0.5,0.6,0.8,1.0,0.01,0.001]\n",
        "Dict_alpha = dict(alpha = alphas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwL3h8-Ld9aZ"
      },
      "source": [
        "## 4.1 Tuning Ridge regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCfFnfIId9aa",
        "outputId": "e0c7a335-0bf7-4812-aa22-f20a6efdc1f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best alpha value for the Ridge model =  0.01\n"
          ]
        }
      ],
      "source": [
        "# Creating instance of Ridge model\n",
        "Model_Ridge = linear_model.Ridge()\n",
        "# Using GridSearchCV to pass the model,values to alpha and cross validation folds \n",
        "Grid_Ridge = GridSearchCV(estimator = Model_Ridge, param_grid=Dict_alpha, cv=10 )\n",
        "# fitting the model on the training set\n",
        "Grid_Ridge.fit(X_train, Y_train)\n",
        "# printing the best alpha value from the given list for the data set in consideration\n",
        "print(\"The best alpha value for the Ridge model = \", Grid_Ridge.best_estimator_.alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5aQOn02d9ac"
      },
      "source": [
        "## 4.2 Tuning Lasso regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7dzLT_3d9ad",
        "outputId": "0154c513-8337-4aa9-f4b6-8e1cc9fb2df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best alpha value for the Lasso model =  0.01\n"
          ]
        }
      ],
      "source": [
        "# Creating instance of Lasso model\n",
        "Model_Lasso = linear_model.Lasso(max_iter=10000)\n",
        "# Using GridSearchCV to pass the model,values to alpha and cross validation folds \n",
        "Grid_Lasso = GridSearchCV(estimator = Model_Lasso, param_grid=Dict_alpha, cv=10 )\n",
        "# fitting the model on the training set\n",
        "Grid_Lasso.fit(X_train, Y_train)\n",
        "# printing the best alpha value from the given list for the data set in consideration\n",
        "print(\"The best alpha value for the Lasso model = \", Grid_Lasso.best_estimator_.alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAy3J3uGd9ad"
      },
      "source": [
        "## 4.3 Tuning Elastic net regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niUD1_PVd9ad",
        "outputId": "cd4d80e6-5d01-456b-f09a-3ae281ff9b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best alpha value for the ElasticNet model =  0.001\n"
          ]
        }
      ],
      "source": [
        "# Creating instance of Elastic net model\n",
        "Model_ElasticNet = linear_model.ElasticNet(max_iter=10000)\n",
        "# Using GridSearchCV to pass the model,values to alpha and cross validation folds\n",
        "Grid_ElasticNet = GridSearchCV(estimator = Model_ElasticNet, param_grid=Dict_alpha, cv=10 )\n",
        "# fitting the model on the training set\n",
        "Grid_ElasticNet.fit(X_train, Y_train)\n",
        "# printing the best alpha value from the given list for the data set in consideration\n",
        "print(\"The best alpha value for the ElasticNet model = \", Grid_ElasticNet.best_estimator_.alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRC_9d5kd9ae"
      },
      "source": [
        "# 5. Tuning Non-Linear Regression models\n",
        "\n",
        "In following codes CART, Knearest neighbor regressor and Support vector  regression models are tuned on parameter defined in the introduction of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhMlldLid9ae"
      },
      "source": [
        "## 5.1 Parameter setting for Support Vector Regressor\n",
        "\n",
        "The model can be tuned on paramteres such as, kernel type, C and gamma. The code below first set the choice of paramters and then GridSearchCV() will be used to search the best combination of parameters for the data in consideration. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpB5vmB8d9ae"
      },
      "outputs": [],
      "source": [
        "# setting paramters\n",
        "param_grid_SVR = [\n",
        "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
        "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
        " ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jcafaf2d9ae"
      },
      "source": [
        "## 5.2 Tuning Support Vector Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u68_QNbd9ah",
        "outputId": "1b7cdd57-4a16-4566-e8a7-9af5fe4f0650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best parameter choice of SVR model SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
            "    gamma='auto_deprecated', kernel='linear', max_iter=-1, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n"
          ]
        }
      ],
      "source": [
        "# Creating instance of SVR model\n",
        "Model_SVR = SVR()\n",
        "# Using GridSearchCV to pass the model,values to parameters for tuning\n",
        "# and cross validation folds\n",
        "Grid_SVR = GridSearchCV(estimator = Model_SVR, param_grid=param_grid_SVR, cv=5)\n",
        "# fitting the model on the training set\n",
        "Grid_SVR.fit(X_train, Y_train)\n",
        "# printing the best parameter values from the given list for the data set in consideration\n",
        "print(\"The best parameter choice of SVR model\", Grid_SVR.best_estimator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xuHJnbId9ai"
      },
      "source": [
        "## 5.3 Parameter setting for CART\n",
        "\n",
        "The model can be tuned on paramteres such as, maxinum depth of tree, maximum leaf and more. The code below first set the choice of paramters and then GridSearchCV() will be used to search the best combination of parameters for the data in consideration. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8YZRRwHd9aj"
      },
      "outputs": [],
      "source": [
        "# setting paramters\n",
        "param_grid_DTR = {\"min_samples_split\": [10, 20, 40],\n",
        "              \"max_depth\": [2, 6, 8],\n",
        "              \"min_samples_leaf\": [20, 40, 100],\n",
        "              \"max_leaf_nodes\": [5, 20, 100],\n",
        "              }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8G6TuSd9aj"
      },
      "source": [
        "## 5.4 Tuning CART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeK4QrOxd9aj"
      },
      "outputs": [],
      "source": [
        "# Creating instance of CART model\n",
        "Model_DTR = DecisionTreeRegressor()\n",
        "# Using GridSearchCV to pass the model,values to parameters for tuning\n",
        "# and cross validation folds\n",
        "Grid_DTR = GridSearchCV(estimator = Model_DTR, param_grid=param_grid_DTR, cv=5)\n",
        "# fitting the model on the training set\n",
        "Grid_DTR.fit(X_train, Y_train)\n",
        "# printing the best parameters from the given list for the data set in consideration\n",
        "print(\"The best parameter choice of Decision tree Regressor model:\", Grid_DTR.best_estimator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP8ehrfMd9aj"
      },
      "source": [
        "## 5.5 Parameter setting for K nearest Neighbour Regressor\n",
        "\n",
        "The model can be tuned on paramteres such as, k NN. The code below first set the choice of paramters and then GridSearchCV() will be used to search the best combination of parameters for the data in consideration. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV0C2Rnjd9ak"
      },
      "outputs": [],
      "source": [
        "# setting paramters\n",
        "param_grid_KNN ={\"n_neighbors\": [5,7,9,11,13,15,17,19,21]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6-Y0jWXd9ak"
      },
      "source": [
        "## 5.6 Tuning K Nearest Neighbour Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7t3vBTWd9ak",
        "outputId": "506edf34-8c4a-4e09-a28c-4f52366690e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best parameter choice of KNN Regressor model: KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                    metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
            "                    weights='uniform')\n"
          ]
        }
      ],
      "source": [
        "# Creating instance of KNN regressor model\n",
        "Model_KNN = KNeighborsRegressor()\n",
        "# Using GridSearchCV to pass the model,values to parameters for tuning\n",
        "# and cross validation folds\n",
        "Grid_KNN = GridSearchCV(estimator = Model_KNN, param_grid=param_grid_KNN, cv=5)\n",
        "# fitting the model on the training set\n",
        "Grid_KNN.fit(X_train, Y_train)\n",
        "# printing the best parameters from the given list for the data set in consideration\n",
        "print(\"The best parameter choice of KNN Regressor model:\", Grid_KNN.best_estimator_)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}